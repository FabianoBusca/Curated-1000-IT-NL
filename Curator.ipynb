{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load the Italian-Dutch parallel corpus",
   "id": "84f495f860283978"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T23:23:02.134886Z",
     "start_time": "2024-12-01T23:23:02.065629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "columns = ['it_id', 'italian', 'nl_id', 'dutch']\n",
    "df = pd.read_csv('Datasets/Tatoeba-it-nl.tsv', sep='\\t', header=None)\n",
    "df.columns = columns\n",
    "df.head()"
   ],
   "id": "47f9f3a60c6a7c5d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   it_id                           italian   nl_id  \\\n",
       "0   4369            Devo andare a dormire.    5966   \n",
       "1   4371                        Che cos'è?    5970   \n",
       "2   4373  La parola d'accesso è \"Muiriel\".    5985   \n",
       "3   4375              Non cambierà niente.  379556   \n",
       "4   4376              Costerà trenta euro.  378907   \n",
       "\n",
       "                                   dutch  \n",
       "0                   Ik moet gaan slapen.  \n",
       "1                            Wat is dat?  \n",
       "2           Het wachtwoord is \"Muiriel\".  \n",
       "3  Dat zal niets aan de zaak veranderen.  \n",
       "4                 Dat zal € 30,- kosten.  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>it_id</th>\n",
       "      <th>italian</th>\n",
       "      <th>nl_id</th>\n",
       "      <th>dutch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4369</td>\n",
       "      <td>Devo andare a dormire.</td>\n",
       "      <td>5966</td>\n",
       "      <td>Ik moet gaan slapen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4371</td>\n",
       "      <td>Che cos'è?</td>\n",
       "      <td>5970</td>\n",
       "      <td>Wat is dat?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4373</td>\n",
       "      <td>La parola d'accesso è \"Muiriel\".</td>\n",
       "      <td>5985</td>\n",
       "      <td>Het wachtwoord is \"Muiriel\".</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4375</td>\n",
       "      <td>Non cambierà niente.</td>\n",
       "      <td>379556</td>\n",
       "      <td>Dat zal niets aan de zaak veranderen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4376</td>\n",
       "      <td>Costerà trenta euro.</td>\n",
       "      <td>378907</td>\n",
       "      <td>Dat zal € 30,- kosten.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Define function to count the phenomena frequencies in the dataset",
   "id": "5ce2367abfe7d3c0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T23:23:18.471726Z",
     "start_time": "2024-12-01T23:23:02.135896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import stanza\n",
    "\n",
    "# Load the Italian language model in stanza\n",
    "stanza.download('it')\n",
    "nlp = stanza.Pipeline(lang='it', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "# Helper functions for phenomena detection\n",
    "def has_subject_omission(sentence):\n",
    "    \"\"\"Check if the first word of the sentence is a verb.\"\"\"\n",
    "    if not isinstance(sentence, str):\n",
    "        return False\n",
    "    doc = nlp(sentence)\n",
    "    first_word_pos = doc.sentences[0].words[0].upos if doc.sentences and doc.sentences[0].words else None\n",
    "    return first_word_pos == \"VERB\" or first_word_pos == \"AUX\"\n",
    "\n",
    "def has_reflexive_construction(sentence):\n",
    "    \"\"\"Check if the sentence contains reflexive pronouns.\"\"\"\n",
    "    reflexive_pronouns = [\"mi\", \"ti\", \"si\", \"ci\", \"vi\"]\n",
    "    return any(pronoun in sentence.split() for pronoun in reflexive_pronouns)\n",
    "\n",
    "def has_double_negation(sentence):\n",
    "    \"\"\"Check if the sentence contains double negation.\"\"\"\n",
    "    negative_words = [\"niente\", \"nessuno\", \"nessuna\", \"nessun\", \"nulla\"]\n",
    "    words = sentence.lower().split()\n",
    "    return words.count(\"non\") > 1 or \"non\" in words and any(word in words for word in negative_words)\n",
    "\n",
    "with open(\"./Datasets/Diminutives-Augmentatives.txt\", 'r', encoding='utf-8') as file:\n",
    "    diminutives_augmentatives = [line.strip() for line in file]\n",
    "\n",
    "def has_diminutives_or_augmentatives(sentence):\n",
    "    \"\"\"Check if the sentence contains diminutives or augmentatives.\"\"\"\n",
    "    suffixes = [\"ino\", \"ina\", \"etto\", \"etta\", \"accio\", \"accia\", \"one\", \"ona\"]\n",
    "    sentence_no_punct = sentence.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return any(word for word in sentence_no_punct.split() if word in diminutives_augmentatives)\n",
    "\n",
    "def has_clitic_pronouns(sentence):\n",
    "    \"\"\"Check if the sentence contains clitic pronouns.\"\"\"\n",
    "    clitic_pronouns = [\"lo\", \"la\", \"li\", \"gli\", \"le\", \"ne\", \"ci\", \"mi\", \"ti\",\n",
    "    \"glielo\", \"gliela\", \"glieli\", \"gliele\"]\n",
    "    return any(pronoun in sentence.split() for pronoun in clitic_pronouns)\n",
    "\n",
    "def phenomena_counter(df):\n",
    "  phenomena_counts = {\n",
    "    \"Rows\": len(df),\n",
    "    \"Sentences\": len(df['it_id'].unique()),\n",
    "    \"Subject Omission\": int(df['subject_omission'].sum()),\n",
    "    \"Reflexive Construction\": int(df['reflexive_construction'].sum()),\n",
    "    \"Double Negation\": int(df['double_negation'].sum()),\n",
    "    \"Diminutives and Augmentatives\": int(df['diminutives_augmentatives'].sum()),\n",
    "    \"Clitic Pronouns\": int(df['clitic_pronouns'].sum()),\n",
    "    \"Other\": len(df[~(df[['subject_omission', 'reflexive_construction', 'double_negation',\n",
    "                          'diminutives_augmentatives', 'clitic_pronouns']].any(axis=1))])\n",
    "  }\n",
    "\n",
    "  return phenomena_counts"
   ],
   "id": "69c34fcffecb70d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b3b281110d704b3c8351229dbea07cd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 00:23:09 INFO: Downloaded file to C:\\Users\\fabia\\stanza_resources\\resources.json\n",
      "2024-12-02 00:23:09 INFO: Downloading default packages for language: it (Italian) ...\n",
      "2024-12-02 00:23:11 INFO: File exists: C:\\Users\\fabia\\stanza_resources\\it\\default.zip\n",
      "2024-12-02 00:23:14 INFO: Finished downloading models and saved to C:\\Users\\fabia\\stanza_resources\n",
      "2024-12-02 00:23:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "536598bb9c9b4566addb2d29358bc4d1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 00:23:15 INFO: Downloaded file to C:\\Users\\fabia\\stanza_resources\\resources.json\n",
      "2024-12-02 00:23:15 INFO: Loading these models for language: it (Italian):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-12-02 00:23:15 INFO: Using device: cpu\n",
      "2024-12-02 00:23:15 INFO: Loading: tokenize\n",
      "C:\\Users\\fabia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-02 00:23:17 INFO: Loading: mwt\n",
      "C:\\Users\\fabia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-02 00:23:17 INFO: Loading: pos\n",
      "C:\\Users\\fabia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "C:\\Users\\fabia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "C:\\Users\\fabia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-02 00:23:18 INFO: Loading: lemma\n",
      "C:\\Users\\fabia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-12-02 00:23:18 INFO: Done loading processors!\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T23:39:21.543958Z",
     "start_time": "2024-12-01T23:23:18.472772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "tqdm.pandas()\n",
    "\n",
    "# Update functions to use tqdm for progress tracking\n",
    "df['subject_omission'] = df['italian'].progress_apply(has_subject_omission)\n",
    "df['reflexive_construction'] = df['italian'].progress_apply(has_reflexive_construction)\n",
    "df['double_negation'] = df['italian'].progress_apply(has_double_negation)\n",
    "df['diminutives_augmentatives'] = df['italian'].progress_apply(has_diminutives_or_augmentatives)\n",
    "df['clitic_pronouns'] = df['italian'].progress_apply(has_clitic_pronouns)\n",
    "\n",
    "df.to_csv('Datasets/Analysis-IT-NL.tsv', sep='\\t', index=False)"
   ],
   "id": "287fdefdbe63f821",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 13481/17525 [16:01<04:48, 14.02it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m tqdm\u001B[38;5;241m.\u001B[39mpandas()\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Update functions to use tqdm for progress tracking\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msubject_omission\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mitalian\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprogress_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhas_subject_omission\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      8\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreflexive_construction\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitalian\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mprogress_apply(has_reflexive_construction)\n\u001B[0;32m      9\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdouble_negation\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mitalian\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mprogress_apply(has_double_negation)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:917\u001B[0m, in \u001B[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001B[1;34m(df, func, *args, **kwargs)\u001B[0m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;66;03m# Apply the provided function (in **kwargs)\u001B[39;00m\n\u001B[0;32m    915\u001B[0m \u001B[38;5;66;03m# on the df using our wrapper (which provides bar updating)\u001B[39;00m\n\u001B[0;32m    916\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf_function\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mwrapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    919\u001B[0m     t\u001B[38;5;241m.\u001B[39mclose()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[0;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4800\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[0;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[1;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[0;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[0;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[0;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[0;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[1;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[1;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[1;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[0;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[0;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[0;32m   1747\u001B[0m     )\n",
      "File \u001B[1;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\std.py:912\u001B[0m, in \u001B[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    906\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapper\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    907\u001B[0m     \u001B[38;5;66;03m# update tbar correctly\u001B[39;00m\n\u001B[0;32m    908\u001B[0m     \u001B[38;5;66;03m# it seems `pandas apply` calls `func` twice\u001B[39;00m\n\u001B[0;32m    909\u001B[0m     \u001B[38;5;66;03m# on the first column/row to decide whether it can\u001B[39;00m\n\u001B[0;32m    910\u001B[0m     \u001B[38;5;66;03m# take a fast or slow code path; so stop when t.total==t.n\u001B[39;00m\n\u001B[0;32m    911\u001B[0m     t\u001B[38;5;241m.\u001B[39mupdate(n\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m t\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mn \u001B[38;5;241m<\u001B[39m t\u001B[38;5;241m.\u001B[39mtotal \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m--> 912\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[2], line 12\u001B[0m, in \u001B[0;36mhas_subject_omission\u001B[1;34m(sentence)\u001B[0m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(sentence, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m doc \u001B[38;5;241m=\u001B[39m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     13\u001B[0m first_word_pos \u001B[38;5;241m=\u001B[39m doc\u001B[38;5;241m.\u001B[39msentences[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mwords[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mupos \u001B[38;5;28;01mif\u001B[39;00m doc\u001B[38;5;241m.\u001B[39msentences \u001B[38;5;129;01mand\u001B[39;00m doc\u001B[38;5;241m.\u001B[39msentences[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mwords \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m first_word_pos \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVERB\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m first_word_pos \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAUX\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\pipeline\\core.py:480\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[1;34m(self, doc, processors)\u001B[0m\n\u001B[0;32m    479\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, doc, processors\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 480\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprocessors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\pipeline\\core.py:431\u001B[0m, in \u001B[0;36mPipeline.process\u001B[1;34m(self, doc, processors)\u001B[0m\n\u001B[0;32m    429\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessors\u001B[38;5;241m.\u001B[39mget(processor_name):\n\u001B[0;32m    430\u001B[0m         process \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessors[processor_name]\u001B[38;5;241m.\u001B[39mbulk_process \u001B[38;5;28;01mif\u001B[39;00m bulk \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocessors[processor_name]\u001B[38;5;241m.\u001B[39mprocess\n\u001B[1;32m--> 431\u001B[0m         doc \u001B[38;5;241m=\u001B[39m \u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    432\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m doc\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\pipeline\\pos_processor.py:85\u001B[0m, in \u001B[0;36mPOSProcessor.process\u001B[1;34m(self, document)\u001B[0m\n\u001B[0;32m     83\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, b \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(batch):\n\u001B[0;32m     84\u001B[0m         idx\u001B[38;5;241m.\u001B[39mextend(b[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m---> 85\u001B[0m         preds \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     87\u001B[0m preds \u001B[38;5;241m=\u001B[39m unsort(preds, idx)\n\u001B[0;32m     88\u001B[0m dataset\u001B[38;5;241m.\u001B[39mdoc\u001B[38;5;241m.\u001B[39mset([doc\u001B[38;5;241m.\u001B[39mUPOS, doc\u001B[38;5;241m.\u001B[39mXPOS, doc\u001B[38;5;241m.\u001B[39mFEATS], [y \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m preds \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m x])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\pos\\trainer.py:98\u001B[0m, in \u001B[0;36mTrainer.predict\u001B[1;34m(self, batch, unsort)\u001B[0m\n\u001B[0;32m     96\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m     97\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m word\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 98\u001B[0m _, preds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mword\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwordchars\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwordchars_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mupos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mxpos\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mufeats\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpretrained\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mword_orig_idx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msentlens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwordlens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     99\u001B[0m upos_seqs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mupos\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munmap(sent) \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m preds[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()]\n\u001B[0;32m    100\u001B[0m xpos_seqs \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxpos\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39munmap(sent) \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m preds[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mtolist()]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\pos\\model.py:169\u001B[0m, in \u001B[0;36mTagger.forward\u001B[1;34m(self, word, word_mask, wordchars, wordchars_mask, upos, xpos, ufeats, pretrained, word_orig_idx, sentlens, wordlens, text)\u001B[0m\n\u001B[0;32m    166\u001B[0m     all_forward_chars \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcharmodel_forward_transform(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m all_forward_chars]\n\u001B[0;32m    167\u001B[0m all_forward_chars \u001B[38;5;241m=\u001B[39m pack(pad_sequence(all_forward_chars, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m--> 169\u001B[0m all_backward_chars \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharmodel_backward\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_char_representation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcharmodel_backward_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    171\u001B[0m     all_backward_chars \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcharmodel_backward_transform(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m all_backward_chars]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:222\u001B[0m, in \u001B[0;36mCharacterLanguageModel.build_char_representation\u001B[1;34m(self, sentences)\u001B[0m\n\u001B[0;32m    219\u001B[0m chars \u001B[38;5;241m=\u001B[39m get_long_tensor(chars, \u001B[38;5;28mlen\u001B[39m(all_data), pad_id\u001B[38;5;241m=\u001B[39mvocab\u001B[38;5;241m.\u001B[39munit2id(CHARLM_END))\u001B[38;5;241m.\u001B[39mto(device\u001B[38;5;241m=\u001B[39mdevice)\n\u001B[0;32m    221\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 222\u001B[0m     output, _, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mchars\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchar_lens\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    223\u001B[0m     res \u001B[38;5;241m=\u001B[39m [output[i, offsets] \u001B[38;5;28;01mfor\u001B[39;00m i, offsets \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(char_offsets)]\n\u001B[0;32m    224\u001B[0m     res \u001B[38;5;241m=\u001B[39m unsort(res, orig_idx)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\char_model.py:153\u001B[0m, in \u001B[0;36mCharacterLanguageModel.forward\u001B[1;34m(self, chars, charlens, hidden)\u001B[0m\n\u001B[0;32m    150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m hidden \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m: \n\u001B[0;32m    151\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcharlstm_h_init\u001B[38;5;241m.\u001B[39mexpand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar_num_layers\u001B[39m\u001B[38;5;124m'\u001B[39m], batch_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar_hidden_dim\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mcontiguous(),\n\u001B[0;32m    152\u001B[0m               \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcharlstm_c_init\u001B[38;5;241m.\u001B[39mexpand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar_num_layers\u001B[39m\u001B[38;5;124m'\u001B[39m], batch_size, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mchar_hidden_dim\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mcontiguous())\n\u001B[1;32m--> 153\u001B[0m output, hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcharlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[43membs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcharlens\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(pad_packed_sequence(output, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m    155\u001B[0m decoded \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecoder(output)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stanza\\models\\common\\packed_lstm.py:22\u001B[0m, in \u001B[0;36mPackedLSTM.forward\u001B[1;34m(self, input, lengths, hx)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28minput\u001B[39m, PackedSequence):\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m pack_padded_sequence(\u001B[38;5;28minput\u001B[39m, lengths, batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)\n\u001B[1;32m---> 22\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpad:\n\u001B[0;32m     24\u001B[0m     res \u001B[38;5;241m=\u001B[39m (pad_packed_sequence(res[\u001B[38;5;241m0\u001B[39m], batch_first\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first)[\u001B[38;5;241m0\u001B[39m], res[\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1135\u001B[0m, in \u001B[0;36mLSTM.forward\u001B[1;34m(self, input, hx)\u001B[0m\n\u001B[0;32m   1123\u001B[0m     result \u001B[38;5;241m=\u001B[39m _VF\u001B[38;5;241m.\u001B[39mlstm(\n\u001B[0;32m   1124\u001B[0m         \u001B[38;5;28minput\u001B[39m,\n\u001B[0;32m   1125\u001B[0m         hx,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbatch_first,\n\u001B[0;32m   1133\u001B[0m     )\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1135\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_VF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlstm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1136\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1137\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_sizes\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_weights\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1140\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1142\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1144\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbidirectional\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1145\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1146\u001B[0m output \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1147\u001B[0m hidden \u001B[38;5;241m=\u001B[39m result[\u001B[38;5;241m1\u001B[39m:]\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "phenomena_counter(df)",
   "id": "6d7fc96a3baf4c20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Check Diminutives and Augmentatives",
   "id": "59745b49cca0e9be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "diminutives_augmentatives = [\n",
    "    word\n",
    "    for words in df[df['diminutives_augmentatives']]['italian'].str.split()\n",
    "    for word in words\n",
    "    if word.endswith(\"ino\") or word.endswith(\"etto\") or word.endswith(\"accio\") or word.endswith(\"one\")\n",
    "]\n",
    "\n",
    "set(diminutives_augmentatives)"
   ],
   "id": "81a8d437cf65ccab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Adjusting the flag based on the actual diminutives and augmentatives",
   "id": "2c0aa4f84450429a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "actual_diminutives_augmentatives = [\"L'amichetto\", 'laghetto', 'modellino', 'motorino', 'orsetto', 'ragazzino']\n",
    "\n",
    "df.loc[\n",
    "    df['diminutives_augmentatives'] & ~df['italian'].str.contains('|'.join(actual_diminutives_augmentatives), na=False),\n",
    "    'diminutives_augmentatives'\n",
    "] = False\n",
    "phenomena_counter(df)"
   ],
   "id": "254b491023700dc2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Split the dataset into single and multiple translations",
   "id": "fd443d67752b5cec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "multiple_translation_df = df[df['nl_id'].isin(df['nl_id'].value_counts()[lambda x: x > 1].index)]\n",
    "phenomena_counter(multiple_translation_df)"
   ],
   "id": "7edde88fdc547de5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df[~df.index.isin(multiple_translation_df.index)]\n",
    "phenomena_counter(df)"
   ],
   "id": "bc5ce3fa92a1f63f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create a curated dataset with a balanced distribution of phenomena",
   "id": "5ad1b6f6d90d912e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def sample_and_remove(source_df, target_df, column_name, n):\n",
    "    if len(source_df[source_df[column_name]]) < n:\n",
    "        n = len(source_df[source_df[column_name]])\n",
    "    sample = source_df[source_df[column_name]].sample(n=n, replace=False)\n",
    "    source_df = source_df.drop(sample.index)\n",
    "    target_df = pd.concat([target_df, sample], ignore_index=True)\n",
    "    return source_df, target_df, n\n",
    "\n",
    "curated_df = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "phenomena = [\n",
    "    'subject_omission',\n",
    "    'reflexive_construction',\n",
    "    'double_negation',\n",
    "    'diminutives_augmentatives',\n",
    "    'clitic_pronouns'\n",
    "]\n",
    "\n",
    "for phenomenon in phenomena:\n",
    "    multiple_translation_df, curated_df, n = sample_and_remove(multiple_translation_df, curated_df, phenomenon, 60)\n",
    "    df, curated_df, m = sample_and_remove(df, curated_df, phenomenon, 120 - n)\n",
    "    if m + n != 120:\n",
    "      print(f\"Missing {120 - m - n} sentences for {phenomenon}\\n\")"
   ],
   "id": "f5eb2e899b80171f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Add the generated sentences",
   "id": "42a80d31285df0eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "generated_df = pd.read_csv('Datasets/Double-Negation-Generated-IT-NL.tsv', sep='\\t')\n",
    "tmp = pd.read_csv('Datasets/Diminutives-Augmentatives-Generated-IT-NL.tsv', sep='\\t')\n",
    "generated_df = pd.concat([generated_df, tmp], ignore_index=True)\n",
    "\n",
    "for _, row in generated_df.iterrows():\n",
    "    sentence = row['italian']\n",
    "    translation = row['dutch']\n",
    "    \n",
    "    if sentence in curated_df['italian'].values:\n",
    "        it_id = curated_df.loc[curated_df['italian'] == sentence, 'it_id'].values[0]\n",
    "    else:\n",
    "        it_id = curated_df['it_id'].max() + 1 if not curated_df.empty else 1  \n",
    "    \n",
    "    if translation in curated_df['dutch'].values:\n",
    "        nl_id = curated_df.loc[curated_df['dutch'] == translation, 'nl_id'].values[0]\n",
    "    else:\n",
    "        nl_id = curated_df['nl_id'].max() + 1 if not curated_df.empty else 1\n",
    "\n",
    "    new_row = pd.DataFrame({'it_id': [it_id], 'italian': [sentence], 'nl_id': [nl_id], 'dutch': [translation]})\n",
    "    curated_df = pd.concat([curated_df, new_row], ignore_index=True)"
   ],
   "id": "fce75a585698f6dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Add random sentences for variety",
   "id": "b8d25141c1929338"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "curated_df = pd.concat([curated_df, df.sample(n=400, replace=False)], ignore_index=True)\n",
    "len(curated_df)"
   ],
   "id": "278f985b00125357",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save the curated dataset",
   "id": "a2f96d72ad84c5ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "curated_df.drop(columns=[\n",
    "    'subject_omission',\n",
    "    'reflexive_construction',\n",
    "    'double_negation',\n",
    "    'diminutives_augmentatives',\n",
    "    'clitic_pronouns'\n",
    "], inplace=True, errors='ignore')\n",
    "curated_df.to_csv('Datasets/Curated-1000-IT-NL.tsv', sep='\\t', index=False)"
   ],
   "id": "91b79a7dfdfcd129",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
